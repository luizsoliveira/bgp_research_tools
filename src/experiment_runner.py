from dotenv import dotenv_values
import json
import os
import utils
from datetime import datetime
from math import floor, ceil
import json
import sys
import logging
import multiprocessing
from netscience_client import NetScienceClient
from dataset.dataset import Dataset
from feature_selection.feature_selection import ExtraTreesFeatureSelection
from exploratory_data_analysis.cli import execute_eda_single_task
import sweetviz as sv

#ConfiguraÃ§Ã£o de LOGGING
logging.basicConfig(
    filename=f"bgpresearch.log",
    filemode='a',
    level=logging.DEBUG,
    format='%(asctime)s %(levelname)-8s %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S')

app_path = f"{os.path.dirname(__file__)}"

# LOADING ENV FILE
netscience_path = f"{app_path}/../netscience.env"
if not os.path.exists(netscience_path):
    msg=f"ABORTING: were not found the netscience environment file at: {netscience_path}"
    logging.error(msg)
    sys.exit(msg)

uname = os.uname()
print(f" ðŸ–¥ï¸ Node Hostname: {uname.nodename}")
print(f"    Sysname: {uname.sysname}")
print(f"    Release: {uname.release}")
print(f"    Version: {uname.version}")
print(f"    Arch: {uname.machine}")

number_of_cores = multiprocessing.cpu_count()
print(f" ðŸ§  Detected number of CPU cores: {number_of_cores}")

netscience_config = dotenv_values(netscience_path)
print(f" ðŸ’¼ Netscience User: {netscience_config['USERNAME']}")

task_working_dir = os.getcwd()
print(f" ðŸ“‚ Starting experiment task on CWD: {task_working_dir}")

today = datetime.today()
print(f" ðŸ•£ Starting time: {today}")

file = open('task.json')
task = json.load(file)

print(f" ðŸ”‘ Task key: {task['id']}")

print(f" ðŸ§ª Getting experiment details...")
client = NetScienceClient(netscience_config['BASE_URL'], netscience_config['USERNAME'], netscience_config['PASSWORD'],netscience_config['TASKS_BASE_PATH'], logging=logging)
task = client.get_experiment(task['id'])

p = task['parameters']
p['dataset_id'] = task['dataset_id']
dataset_task_path = os.path.join(netscience_config['TASKS_BASE_PATH'], task['dataset_id'])
p['dataset_path'] = os.path.join(dataset_task_path, 'DATASET.csv')

print(f" âš™ï¸ Task parameters (before parsing):")
utils.print_task_parameters(task)

# Preparing the parameters
dataset_id = p['dataset_id']
dataset_path = p['dataset_path']

data_partition_training = p['data_partition_training'] / 100

rnn_length = p['rnn_length']

debug = True if p['debug'] == 'activated' else False

params = {
    'dataset_id': dataset_id,
    'dataset_path': dataset_path,
    'data_partition_training': data_partition_training,
    'rnn_length': rnn_length,
    'debug': debug,
}

print('')

print(f" âš™ï¸ Task parameters (after parsing):")
utils.print_generic_parameters(params)

print('')

print(f" ðŸ• Data partition:")

print(f"  â†’ Train size (proportion): {data_partition_training}")
print()

#Dataset object
dataset = Dataset(dataset_path)
#Dataset DataFrame
df = dataset.df

print(f" â›”ï¸ Looking for NaN values")
nan_rows = df[df.isna().any(axis=1)]
if (len(nan_rows)> 0):
    print(f"  â†’ {nan_rows} NaN values were found.")
else: print(f"  â†’ No NaN value were found.")
print()

print(f" ðŸ“Š Stats of the original dataset")
print(f"  â†’ Regular: {len(df.loc[df['LABEL'] == 0])}")
print(f"  â†’ Anomalous: {len(df.loc[df['LABEL'] == 1])}")
print(f"  â†’ Total: {len(df)}")
print(f"  â†’ NaN lines: {len(nan_rows)}")
df = df.dropna()
print(f"  â†’ Total after remove NaN lines: {len(df)}")
print()

df.info(verbose=True, show_counts=True)
print()

#Train Dataset
tr_dataset = dataset.get_training_sample(data_partition_training)

print(f" ðŸ“Š Stats of the TRAINING sample:")
print(f"  â†’ Regular: {tr_dataset.count_regular_data_points()}")
print(f"  â†’ Anomalous: {tr_dataset.count_anomalous_data_points()}")
print(f"  â†’ Total: {tr_dataset.count_total_data_points()}")
print()

tr_dataset.df.info(verbose=True, show_counts=True)
print()

#Testing Dataset
ts_dataset = dataset.get_testing_sample(data_partition_training)
print(f" ðŸ“Š Stats of the TESTING sample:")
print(f"  â†’ Regular: {ts_dataset.count_regular_data_points()}")
print(f"  â†’ Anomalous: {ts_dataset.count_anomalous_data_points()}")
print(f"  â†’ Total: {ts_dataset.count_total_data_points()}")
print()

ts_dataset.df.info(verbose=True, show_counts=True)
print()

print(f" ðŸ“‰ Executing automatic exploratory data analysis")

html_path = os.path.join(task_working_dir, 'eda.html')
report = sv.compare([tr_dataset.df, "Trainning"],[ts_dataset.df, "Test"], target_feat=dataset.target_column)
try:
    report.show_html(html_path, open_browser=False, layout='vertical')
except Exception as e:
    print(f"Failure during writing html EDA file. {html_path}")

